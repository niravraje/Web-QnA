{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('val_questions.json', 'r') as fp:\n",
    "    val_questions = json.load(fp)\n",
    "    \n",
    "with open('val_answers.json', 'r') as fp:\n",
    "    val_answers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import urllib.request\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_article_text(url):\n",
    "    # Scrape article using bs4 to extract all paragraphs from the online article.\n",
    "    raw_html = urllib.request.urlopen(url)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "    # Creating a document 'article_text' containing all the sentences in the article.\n",
    "    article_text = ''\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "    return article_text\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    filtered_sentence = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for token in word_tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_sentence.append(token)\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_article_text(article_text):\n",
    "    # Creating a corpus containing all the sentence tokens in the document.\n",
    "    corpus = nltk.sent_tokenize(article_text)\n",
    "    # Convert to lowercase, remove non-word characters (punctuations, etc.) and strip whitespaces\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = clean_sentence(corpus[i])\n",
    "    return corpus\n",
    "\n",
    "def create_word_freq_dictionary(corpus):\n",
    "    # Create dictionary with word frequency\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for token in word_tokens:\n",
    "            word_freq[token] += 1\n",
    "    return word_freq\n",
    "\n",
    "def generate_sent_vec(sentence, most_freq_tokens):\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq_tokens:\n",
    "        if token in word_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    return sent_vec\n",
    "\n",
    "def get_sentence_vectors(corpus, most_freq_tokens):\n",
    "    # Generate sentence vectors of 1's and 0's. Feature set is the most_freq_tokens list.\n",
    "    sentence_vectors = []\n",
    "    for sentence in corpus:\n",
    "        sent_vec = generate_sent_vec(sentence, most_freq_tokens)\n",
    "        sentence_vectors.append(sent_vec)\n",
    "        \n",
    "    sentence_vectors = np.asarray(sentence_vectors)\n",
    "    return sentence_vectors\n",
    "\n",
    "def get_answer(url, question):\n",
    "\n",
    "    article_text = get_article_text(url)\n",
    "    #print(\"Article Text: \\n\", article_text)\n",
    "    initial_corpus = nltk.sent_tokenize(article_text)\n",
    "    corpus = clean_article_text(article_text)\n",
    "\n",
    "    word_freq = create_word_freq_dictionary(corpus)\n",
    "\n",
    "    # Get the most frequent tokens from the dictionary\n",
    "    most_freq_tokens = heapq.nlargest(200, word_freq, key=word_freq.get)\n",
    "\n",
    "    sentence_vectors = get_sentence_vectors(corpus, most_freq_tokens)\n",
    "\n",
    "    cleaned_question = clean_sentence(question)\n",
    "    question_vector = generate_sent_vec(cleaned_question, most_freq_tokens)\n",
    "\n",
    "    similarity_scores = []\n",
    "    sent_vec_index = 0\n",
    "    for sent_vec in sentence_vectors:\n",
    "        similarity = 1 - sp.spatial.distance.cosine(question_vector, sent_vec)\n",
    "        similarity_scores.append((sent_vec_index, similarity))\n",
    "        sent_vec_index += 1\n",
    "    similarity_scores.sort(key = lambda x: x[1], reverse=True)\n",
    "    answer_index = similarity_scores[0][0]\n",
    "\n",
    "    return initial_corpus[answer_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the word frequency dictionary, most freq tokens and sentence vectors for the given article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/India'\n",
    "\n",
    "article_text = get_article_text(url)\n",
    "#print(\"Article Text: \\n\", article_text)\n",
    "\n",
    "# Maintaining initial corpus for displaying answers:\n",
    "initial_corpus = nltk.sent_tokenize(article_text)\n",
    "# Clean the article sentence to remove extra whitespaces and reference numbers (such as \"[23]\")\n",
    "for i in range(len(initial_corpus)):\n",
    "    initial_corpus[i] = re.sub(r'\\[\\d+\\]', '', initial_corpus[i])\n",
    "    initial_corpus[i] = re.sub(r'\\[\\d+,\\s\\d+]', '', initial_corpus[i])\n",
    "    initial_corpus[i] = re.sub(r'\\[\\w\\]', '', initial_corpus[i])\n",
    "    initial_corpus[i] = re.sub(r'\\s+', ' ', initial_corpus[i]).strip()\n",
    "\n",
    "# Generating a clean corpus to be fed to the model.\n",
    "corpus = clean_article_text(article_text)\n",
    "\n",
    "word_freq = create_word_freq_dictionary(corpus)\n",
    "\n",
    "# Get the most frequent tokens from the dictionary\n",
    "most_freq_tokens = heapq.nlargest(200, word_freq, key=word_freq.get)\n",
    "\n",
    "sentence_vectors = get_sentence_vectors(corpus, most_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence vectors are 0 and 1 vectors which have the most_freq_tokens as their feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence vector: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Val questions:\n",
      "[('0', 'What is India?'), ('1', 'When did modern humans arrive on the Indian subcontinent?'), ('10', 'When did the name Bharat gain increased currency as a native name for India?')]\n",
      "\n",
      "Val answers:\n",
      "[('0', 'India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya), is a country in South Asia.'), ('1', 'Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.'), ('2', 'The Dravidian languages of India were supplanted in the northern and western regions.')]\n"
     ]
    }
   ],
   "source": [
    "print('\\nSentence vector: {}'.format(sentence_vectors[4]))\n",
    "print('\\nVal questions:\\n{}'.format(list(val_questions.items())[:3]))\n",
    "print('\\nVal answers:\\n{}'.format(list(val_answers.items())[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict answers on for the val_questions and save the predictions in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niraje\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "pred_answers = dict()\n",
    "for qid, question in val_questions.items():\n",
    "    cleaned_question = clean_sentence(question)\n",
    "    question_vector = generate_sent_vec(cleaned_question, most_freq_tokens)\n",
    "\n",
    "    similarity_scores = []\n",
    "    sent_vec_index = 0\n",
    "    for sent_vec in sentence_vectors:\n",
    "        similarity = 1 - sp.spatial.distance.cosine(question_vector, sent_vec)\n",
    "        similarity_scores.append((sent_vec_index, similarity))\n",
    "        sent_vec_index += 1\n",
    "    similarity_scores.sort(key = lambda x: x[1], reverse=True)\n",
    "    answer_index = similarity_scores[0][0]\n",
    "\n",
    "    pred_answers[qid] = initial_corpus[answer_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions\n",
    "with open('bow_pred_answers.json', 'w') as fp:\n",
    "    json.dump(pred_answers, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to compute metrics and evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    #print(pred_tokens)\n",
    "    #print(truth_tokens)\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    #print(common_tokens)\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg F1 Score: 0.3711199728699776\n",
      "\n",
      "Avg EM Score: 0.29\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for qid, pred_ans in pred_answers.items():\n",
    "    true_ans = val_answers[qid]\n",
    "    f1_score = compute_f1(pred_ans, true_ans)\n",
    "    em_score = compute_exact_match(pred_ans, true_ans)\n",
    "    \n",
    "    f1_scores.append(f1_score)\n",
    "    em_scores.append(em_score)\n",
    "\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "avg_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "print('\\nAvg F1 Score: {}'.format(avg_f1))\n",
    "print('\\nAvg EM Score: {}'.format(avg_em))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: When did modern humans arrive on the Indian subcontinent?\n",
      "\n",
      "Pred answer: Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.\n",
      "\n",
      "True answer: Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.\n",
      "\n",
      "EM: 1\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "qid = '1'\n",
    "\n",
    "print(\"\\nQuestion: {}\".format(val_questions[qid]))\n",
    "print(\"\\nPred answer: {}\".format(pred_answers[qid]))\n",
    "print(\"\\nTrue answer: {}\".format(val_answers[qid]))\n",
    "\n",
    "em = compute_exact_match(pred_answers[qid], val_answers[qid])\n",
    "f1 = compute_f1(pred_answers[qid], val_answers[qid])\n",
    "\n",
    "print(\"\\nEM: {}\".format(em))\n",
    "print(\"F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
