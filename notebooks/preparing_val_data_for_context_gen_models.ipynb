{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import urllib.request\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(url):\n",
    "    # Scrape article using bs4 to extract all paragraphs from the online article.\n",
    "    raw_html = urllib.request.urlopen(url)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "    # Creating a document 'article_text' containing all the sentences in the article.\n",
    "    article_text = ''\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = get_article_text('https://en.wikipedia.org/wiki/India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article_text(article_text):\n",
    "    # Creating a corpus containing all the sentence tokens in the document.\n",
    "    corpus = nltk.sent_tokenize(article_text)\n",
    "    # Convert to lowercase, remove non-word characters (punctuations, etc.) and strip whitespaces\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = clean_sentence(corpus[i])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize article text into sentences.\n",
    "article_sentences = nltk.sent_tokenize(article_text)\n",
    "\n",
    "# Clean the article sentence to remove extra whitespaces and reference numbers (such as \"[23]\")\n",
    "for i in range(len(article_sentences)):\n",
    "    article_sentences[i] = re.sub(r'\\[\\d+\\]', '', article_sentences[i])\n",
    "    article_sentences[i] = re.sub(r'\\[\\d+,\\s\\d+]', '', article_sentences[i])\n",
    "    article_sentences[i] = re.sub(r'\\[\\w\\]', '', article_sentences[i])\n",
    "    article_sentences[i] = re.sub(r'\\s+', ' ', article_sentences[i]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_article_sentences = article_sentences[::4][:-11]\n",
    "len(val_article_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_answers = dict()\n",
    "#val_questions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, sent in enumerate(val_article_sentences):\\n    val_answers[i] = sent\\n    print('\\nSentence {}: {}'.format(i, sent))\\n    ques = input('Enter question: ')\\n    val_questions[i] = ques\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i, sent in enumerate(val_article_sentences):\n",
    "    val_answers[i] = sent\n",
    "    print('\\nSentence {}: {}'.format(i, sent))\n",
    "    ques = input('Enter question: ')\n",
    "    val_questions[i] = ques\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(val_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_answers = dict()\n",
    "for i, sent in enumerate(val_article_sentences):\n",
    "    val_answers[i] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('val_answers.json', 'w') as fp:\n",
    "    json.dump(val_answers, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('val_questions.json', 'w') as fp:\n",
    "    json.dump(val_questions, fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    print(pred_tokens)\n",
    "    print(truth_tokens)\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    print(common_tokens)\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 'The ancient Greeks referred to the Indians as Indoi (Ἰνδοί), which translates as \"The people of the Indus\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = 'The ancient Greeks referred to the Indians as Indoi (Ἰνδοί), which translates as \"The people of the Indus\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ancient', 'greeks', 'referred', 'to', 'indians', 'as', 'indoi', 'ἰνδοί', 'which', 'translates', 'as', 'people', 'of', 'indus']\n",
      "['ancient', 'greeks', 'referred', 'to', 'indians', 'as', 'indoi', 'ἰνδοί', 'which', 'translates', 'as', 'people', 'of', 'indus']\n",
      "{'ancient', 'translates', 'which', 'to', 'people', 'ἰνδοί', 'referred', 'as', 'indus', 'indoi', 'of', 'greeks', 'indians'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_exact_match(pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
