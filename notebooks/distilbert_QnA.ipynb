{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\niraje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "sys.path.append('C:/Users/niraje/Documents/MLG/Web-QnA/modules')\n",
    "import infersent_glove_context_generation as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_token_type_ids = True)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_phrase(question, context):\n",
    "    '''\n",
    "    Takes a `question` string and an `context` string (which contains the\n",
    "    answer), and identifies the words within the `context` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, context)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    #print('Answer: \"' + answer + '\"')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(url, question):\n",
    "    context = ig.generate_context(url, question)\n",
    "    answer = extract_answer_phrase(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-86b5cb4b8dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_answer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Answer: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-24c73e44da5a>\u001b[0m in \u001b[0;36mfind_answer\u001b[1;34m(url, question)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_answer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_answer_phrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-450add703890>\u001b[0m in \u001b[0;36mextract_answer_phrase\u001b[1;34m(question, context)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Run our example question through the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n\u001b[1;32m---> 33\u001b[1;33m                                     token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# ======== Reconstruct Answer ========\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/India'\n",
    "question = 'Which sports does India play?'\n",
    "\n",
    "start_time = time.time()\n",
    "with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    answer = find_answer(url, question)\n",
    "end_time = time.time()\n",
    "print('Answer: ', answer)\n",
    "print('\\n\\nExecution Time: {} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question  What was President Donald Trump's prediction?\n",
      "\n",
      "Answer Tokens: \n",
      "['some', 'states', 'would', 're', '##open', 'this', 'month']\n",
      "\n",
      "Answer :  some states would reopen this month\n",
      "\n",
      "Execution Time: 0.1276559829711914 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "context = \"The US has passed the peak on new coronavirus cases, \" \\\n",
    "          \"President Donald Trump said and predicted that some states would reopen this month. \" \\\n",
    "          \"The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\"\n",
    "\n",
    "question = \"What was President Donald Trump's prediction?\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(question, context)\n",
    "\n",
    "\n",
    "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)\n",
    "\n",
    "print (\"\\nQuestion \",question)\n",
    "print (\"\\nAnswer Tokens: \")\n",
    "print (answer_tokens)\n",
    "\n",
    "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "print (\"\\nAnswer : \",answer_tokens_to_string)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nExecution Time: {} seconds.\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8419,
     "status": "ok",
     "timestamp": 1598788807663,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "wxE3d-gfstuF",
    "outputId": "099c4d18-cde9-4b8d-9506-c33750cffdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append('/content/drive/My Drive/colab_files/modules')\n",
    "\n",
    "import infersent_glove_context_generation as ig\n",
    "\n",
    "import time\n",
    "import os\n",
    "import contextlib\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23562,
     "status": "ok",
     "timestamp": 1598788822811,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "GcEs7yce17QL"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2113,
     "status": "ok",
     "timestamp": 1598788881821,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "Fzz4gRuH1jlC"
   },
   "outputs": [],
   "source": [
    "def extract_answer_phrase(question, context):\n",
    "    '''\n",
    "    Takes a `question` string and an `context` string (which contains the\n",
    "    answer), and identifies the words within the `context` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, context)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    #print('Answer: \"' + answer + '\"')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2626,
     "status": "ok",
     "timestamp": 1598788882901,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "5RUi4sY74Oih"
   },
   "outputs": [],
   "source": [
    "def find_answer(url, question):\n",
    "    context = ig.generate_context(url, question)\n",
    "    answer = extract_answer_phrase(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAnqRRAATvAJ"
   },
   "source": [
    "**Evaluate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1803,
     "status": "ok",
     "timestamp": 1598788928508,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "7c7c3-fCqmn_"
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/India'\n",
    "question = 'Which sports does India play?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29593,
     "status": "ok",
     "timestamp": 1598789157010,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "a6knyQdWqMZR",
    "outputId": "9adfd260-3a7d-411d-9e42-8338c4215b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  cricket is the most popular sport in india . other sports in which indians have succeeded internationally include badminton ( saina nehwal and p v sindhu are two of the top - ranked female badminton players in the world ) , boxing , and wrestling\n",
      "\n",
      "\n",
      "Execution Time: 27.999518394470215 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    answer = find_answer(url, question)\n",
    "end_time = time.time()\n",
    "print('Answer: ', answer)\n",
    "print('\\n\\nExecution Time: {} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47457,
     "status": "aborted",
     "timestamp": 1598788846737,
     "user": {
      "displayName": "Nirav Raje",
      "photoUrl": "https://lh6.googleusercontent.com/-rwxb81DyHSk/AAAAAAAAAAI/AAAAAAAABco/BzjCAyIBALo/s64/photo.jpg",
      "userId": "10387550002262274732"
     },
     "user_tz": -330
    },
    "id": "GcmusUKe-Kaw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIX0EOh8qn43u+RU1HSsgW",
   "collapsed_sections": [],
   "mount_file_id": "1eFFofUFteVb96QccmbzkL1ojSltHmmZ9",
   "name": "bert_QnA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
