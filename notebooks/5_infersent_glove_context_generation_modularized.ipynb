{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularized code for context generation based on the web page content and the input question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import math\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import urllib.request\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For the model\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_tokens(url):\n",
    "    # Scrape article using bs4 to extract all paragraphs from the online article.\n",
    "    raw_html = urllib.request.urlopen(url)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "    \n",
    "    # Creating a document 'article_text' containing all the sentences in the article.\n",
    "    article_text = ''\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "        \n",
    "    # Tokenize article text into sentences.\n",
    "    article_sentences = nltk.sent_tokenize(article_text)\n",
    "    \n",
    "    # Clean the article sentence to remove extra whitespaces and reference numbers (such as \"[23]\")\n",
    "    for i in range(len(article_sentences)):\n",
    "        article_sentences[i] = re.sub(r'\\[\\d+\\]', '', article_sentences[i])\n",
    "        article_sentences[i] = re.sub(r'\\[\\w\\]', '', article_sentences[i])\n",
    "        article_sentences[i] = re.sub(r'\\s+', ' ', article_sentences[i]).strip()\n",
    "    \n",
    "    return article_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def euclidean_dist(u, v):\n",
    "    return math.sqrt(sum([(a - b) ** 2 for a, b in zip(u, v)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_sentences(question_vector, embeddings, sent_count):\n",
    "    \"\"\"Returns the most similar sentences to the question vector.\n",
    "    Similarity Coefficient used: Cosine Index\n",
    "    Sentence count refers to number of most similar sentences to be returned.\n",
    "    \"\"\"\n",
    "    most_sim_sentences = []\n",
    "    for sent_index, sent_vector in enumerate(embeddings):\n",
    "        most_sim_sentences.append((sent_index, cosine(question_vector, sent_vector))) # appending a tuple\n",
    "    most_sim_sentences.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    assert sent_count <= len(embeddings), 'Enter sent_count value less than or equal to {0}'.format(len(embeddings))\n",
    "    return most_sim_sentences[:sent_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    # Load the InferSent model\n",
    "    model_version = 1\n",
    "    MODEL_PATH = \"../InferSent/encoder/infersent%s.pkl\" % model_version\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "    model = InferSent(params_model)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    \n",
    "    # Set the GloVe directory path:\n",
    "    W2V_PATH = '../word_vectors/glove/glove.6B.300d.txt'\n",
    "    model.set_w2v_path(W2V_PATH)\n",
    "    \n",
    "    # Load embeddings of K most frequent words\n",
    "    model.build_vocab_k_words(K=100000)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(url, question):\n",
    "    \n",
    "    # Get the sentence tokens for the entire article text.\n",
    "    article_sentences = get_sentence_tokens(url)\n",
    "    \n",
    "    # Get the prepared model using GloVe/InferSent embeddings as its vocabulary.\n",
    "    model = prepare_model()\n",
    "    \n",
    "    # Encode sentences\n",
    "    embeddings = model.encode(article_sentences, bsize=128, tokenize=False, verbose=True)\n",
    "    \n",
    "    # Encode the question\n",
    "    question = [question]\n",
    "    question_vector = model.encode(question, bsize=128, tokenize=False, verbose=True)[0]\n",
    "    \n",
    "    # Get most similar \"N\" sentence tokens i.e. sent_count\n",
    "    most_sim_sentences = get_most_similar_sentences(question_vector, embeddings, sent_count = 30)\n",
    "    \n",
    "    # Build context paragraph.\n",
    "    # Choose max_token_count such that total token count (question and context) is < 512.\\\n",
    "    context_list = []\n",
    "    context_token_count = 0\n",
    "    max_token_count = 400\n",
    "\n",
    "    for sent_index, similarity_score in most_sim_sentences:\n",
    "        sent_token_count = len(nltk.word_tokenize(article_sentences[sent_index]))\n",
    "        if context_token_count + sent_token_count < max_token_count:\n",
    "            context_list.append(article_sentences[sent_index])\n",
    "            context_token_count += sent_token_count\n",
    "\n",
    "    context_para = ' '.join(context_list)\n",
    "    \n",
    "    return context_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Nb words kept : 7295/11303 (64.5%)\n",
      "Speed : 50.9 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2/7 (28.6%)\n",
      "Speed : 50.1 sentences/s (cpu mode, bsize=128)\n",
      "\n",
      "Execution time:  29.600441932678223  seconds\n",
      "\n",
      "\n",
      "Context generated:\n",
      " Cricket is the most popular sport in India. Other sports in which Indians have succeeded internationally include badminton (Saina Nehwal and P V Sindhu are two of the top-ranked female badminton players in the world), boxing, and wrestling. In India, several traditional indigenous sports remain fairly popular, such as kabaddi, kho kho, pehlwani and gilli-danda. In 1998, the BJP was able to form a successful coalition, the National Democratic Alliance (NDA). Corruption in India is perceived to have decreased. India has no national language. Major domestic competitions include the Indian Premier League, which is the most-watched cricket league in the world and ranks sixth among all sports leagues. Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself. According to a 2011 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2045. Increasingly, in urban settings in northern India, the sari is no longer the apparel of everyday wear, transformed instead into one for formal occasions. The resulting Mughal Empire did not stamp out the local societies it came to rule. 30.7% of India's children under the age of five are underweight. India is notable for its religious diversity, with Hinduism, Buddhism, Sikhism, Islam, Christianity, and Jainism among the nation's major religions. At the workplace in urban India, and in international or leading Indian companies, caste-related identification has pretty much lost its importance. In its more modern form, it has been used to cover the head, and sometimes the face, as a veil. An example of this dominance is the basketball competition where the Indian team won three out of four tournaments to date. Accord to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care. In 2011, the annual defence budget increased by 11.6%, although this does not include funds that reach the military through other branches of government. India has traditionally been the dominant country at the South Asian Games.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/India'\n",
    "question = 'Which sports does India play?'\n",
    "start_time = time.time()\n",
    "context_para = generate_context(url, question)\n",
    "end_time = time.time()\n",
    "\n",
    "print('\\nExecution time: ', end_time - start_time, ' seconds')\n",
    "\n",
    "print('\\n\\nContext generated:\\n', context_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
