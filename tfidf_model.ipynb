{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **TF-IDF** scheme is a type of bag words approach where instead of adding zeros and ones in the embedding vector, you add floating numbers that contain more useful information compared to zeros and ones. \n",
    "\n",
    "The idea behind TF-IDF scheme is the fact that words having a high frequency of occurrence in one document, and less frequency of occurrence in all the other documents, are more crucial for classification.\n",
    "\n",
    "TF-IDF is a product of two values: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "**Term frequency** refers to the number of times a word appears in the document and can be calculated as:\n",
    "\n",
    "##### Term frequence = (Number of Occurences of a word)/(Total words in the document)</i>\n",
    "\n",
    "**IDF** refers to the log of the total number of documents divided by the number of documents in which the word exists, and can be calculated as:\n",
    "\n",
    "##### IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "\n",
    "**Disadvantages:**\n",
    "Though TF-IDF is an improvement over the simple bag of words approach and yields better results for common NLP tasks, the overall pros and cons remain the same. We still need to create a huge sparse matrix, which also takes a lot more computation than the simple bag of words approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import urllib.request\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_article_text(url):\n",
    "    # Scrape article using bs4 to extract all paragraphs from the online article.\n",
    "    raw_html = urllib.request.urlopen(url)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "    # Creating a document 'article_text' containing all the sentences in the article.\n",
    "    article_text = ''\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "    return article_text\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    filtered_sentence = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for token in word_tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_sentence.append(token)\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for token in word_tokens:\n",
    "        lemm_token = lemmatizer.lemmatize(token)\n",
    "        lemmatized_sentence.append(lemm_token)\n",
    "    lemmatized_sentence = ' '.join(lemmatized_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = lemmatize(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_article_text(article_text):\n",
    "    # Creating a corpus containing all the sentence tokens in the document.\n",
    "    corpus = nltk.sent_tokenize(article_text)\n",
    "    # Convert to lowercase, remove non-word characters (punctuations, etc.) and strip whitespaces\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = clean_sentence(corpus[i])\n",
    "    return corpus\n",
    "\n",
    "def get_most_freq_tokens(corpus):\n",
    "    # Create dictionary with word frequency\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for token in word_tokens:\n",
    "            word_freq[token] += 1\n",
    "    most_freq_tokens = heapq.nlargest(200, word_freq, key=word_freq.get)\n",
    "    return most_freq_tokens\n",
    "  \n",
    "\n",
    "# IDF = log((Total number of sentences (documents)) \n",
    "# divided by\n",
    "# (Number of sentences (documents) containing the word))\n",
    "def compute_idf_values(corpus, most_freq_tokens):\n",
    "    word_idf_values = {}\n",
    "    for token in most_freq_tokens:\n",
    "        sentences_with_word = 0\n",
    "        for sentence in corpus:\n",
    "            if token in nltk.word_tokenize(sentence):\n",
    "                sentences_with_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(sentences_with_word + 1))\n",
    "    return word_idf_values\n",
    "\n",
    "# TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
    "# Unlike IDF values, TF values of a word are different for each sentence in the corpus.\n",
    "# IDF values of a word are the same for each sentence.\n",
    "def compute_tf_values(corpus, most_freq_tokens):\n",
    "    word_tf_values = {}\n",
    "    for token in most_freq_tokens:\n",
    "        sent_tf_vector = []\n",
    "        for sentence in corpus:\n",
    "            wordfreq_in_sent = 0\n",
    "            for word_token in nltk.word_tokenize(sentence):\n",
    "                if word_token == token:\n",
    "                    wordfreq_in_sent += 1\n",
    "            word_tf = wordfreq_in_sent/len(nltk.word_tokenize(sentence))\n",
    "            sent_tf_vector.append(word_tf)\n",
    "\n",
    "        # Storing the tf values of a word for each sentence in the corpus.\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "    return word_tf_values\n",
    "\n",
    "def compute_tfidf_model(corpus, most_freq_tokens):\n",
    "    word_tf_values = compute_tf_values(corpus, most_freq_tokens)\n",
    "    word_idf_values = compute_idf_values(corpus, most_freq_tokens)\n",
    "    tfidf_values = []\n",
    "    \n",
    "    for token in word_tf_values.keys():\n",
    "        tfidf_for_word_sentscores = []\n",
    "        for sent_tf_vector in word_tf_values[token]:\n",
    "            tfidf_scores = sent_tf_vector * word_idf_values[token]\n",
    "            tfidf_for_word_sentscores.append(tfidf_scores)\n",
    "        tfidf_values.append(tfidf_for_word_sentscores)\n",
    "\n",
    "    tfidf_model = np.asarray(tfidf_values)\n",
    "    tfidf_model = np.transpose(tfidf_model)\n",
    "    return tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_model(url):\n",
    "    article_text = get_article_text(url)\n",
    "    initial_corpus = nltk.sent_tokenize(article_text)\n",
    "    corpus = clean_article_text(article_text)\n",
    "    most_freq_tokens = get_most_freq_tokens(corpus)\n",
    "\n",
    "    tuned_tfidf_model = compute_tfidf_model(corpus, most_freq_tokens)\n",
    "    return tuned_tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, url, tuned_tfidf_model):\n",
    "    article_text = get_article_text(url)\n",
    "    initial_corpus = nltk.sent_tokenize(article_text)\n",
    "    corpus = clean_article_text(article_text)\n",
    "    most_freq_tokens = get_most_freq_tokens(corpus)\n",
    "    \n",
    "    cleaned_question = clean_sentence(question)\n",
    "    question_vector = compute_tfidf_model([cleaned_question], most_freq_tokens)\n",
    "    \n",
    "    similarity_scores = []\n",
    "    sent_vec_index = 0\n",
    "    for sent_vec in tuned_tfidf_model:\n",
    "        similarity = 1 - sp.spatial.distance.cosine(question_vector, sent_vec)\n",
    "        similarity_scores.append((sent_vec_index, similarity))\n",
    "        sent_vec_index += 1\n",
    "    similarity_scores.sort(key = lambda x: x[1], reverse=True)\n",
    "    answer_index = similarity_scores[0][0]\n",
    "    answer = initial_corpus[answer_index]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/India'\n",
    "tuned_tfidf_model = get_tuned_model(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya),[20] is a country in South Asia.\n"
     ]
    }
   ],
   "source": [
    "question = 'What did ancient greeks refer to Indians as?'\n",
    "answer = get_answer(question, url, tuned_tfidf_model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
